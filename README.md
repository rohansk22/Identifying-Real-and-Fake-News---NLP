# Identifying-Real-and-Fake-News---NLP
Classifying real and fake news using natural language processing </br>

<br> Dataset used: https://www.kaggle.com/datasets/mayuri789/real-vs-fake-news <br> 

I used two datasets ( real and fake news ) and concatenated them into a single dataframe in pandas. <br> 
<br> **Step 1**: Basic EDA <br> 
<br> **Step 2**: Tokenization: I used word_tokenize from the nltk library to create the tokenized text columns <br> 
<br> **Step 3**: Lower case: I lower-cased the tokenized text column <br> 
<br> **Step 4**: Remove Stop Words: Removed all stop words using stopwords from NLTK <br> 
<br> **Step 5**: Stemming: Stemming refers to the process of reducing words to their base or root form, typically by removing suffixes and prefixes. The purpose of stemming is to normalize words so that variations of the same word are treated as the same word, thereby reducing redundancy in the text and improving the accuracy of analysis.<br> 

The cleaned lower texted non-stop words column was then Stemmed. <br> 

<br> Ex: 'jumping', 'jumped', 'jumper' all come from the base or root word "jump". <br> 

<br> **Step 6**: Lemmatization: Lemmatization is a process in natural language processing (NLP) that involves reducing words to their base or canonical form, known as the lemma. Unlike stemming, which simply chops off prefixes and suffixes to find the root form of a word, lemmatization considers the context and meaning of the word to determine its lemma. <br> 

<br> The cleaned lower texted non-stop words column was then lemmatized <br> 

<br> Ex: The lemma of 'jumping', 'jumped', 'jumper' all come from the base or root word "jump". <br> 

However, Lemmatization tends to be more accurate than stemming but can also be computationally more expensive due to the need for dictionary lookup and morphological analysis. However, because lemmatization returns valid words, it is often preferred over stemming in applications where linguistic accuracy is crucial. <br> 

<br> **Step 6**: Vectorizing: The cleaned text stemmed/lemmatized corpus is vectorized using TF-IDF: Term frequency Inverse Document Frequency. <br> 
<br> ***Term Frequency (TF):*** It measures how frequently a term occurs in a document. It's calculated as the number of times a term appears in a document divided by the total number of terms in the document. TF increases with the frequency of the term within the document. <br>

***Inverse Document Frequency (IDF):*** It measures how important a term is across the entire corpus. It's calculated as the logarithm of the total number of documents divided by the number of documents containing the term. IDF decreases with the number of documents containing the term. <br>

***TF-IDF Score:*** It's calculated by multiplying TF by IDF. The higher the TF-IDF score of a term in a document, the more relevant that term is to the document. <br>

When using a TF-IDF Vectorizer, each document is represented as a vector where each element corresponds to the TF-IDF score of a term in the document.<br>

<br>**Step 7**: Sparse Matrix: The vectorized corpus has many 0s. the TF-IDF matrix generated by the vectorizer is often very high-dimensional and contains mostly zero values because most terms appear in only a subset of documents. This kind of matrix is called a "sparse matrix." <br> 

<br> A sparse matrix stores only non-zero entries to save memory and computational resources. It's an efficient way of representing and manipulating matrices with a large number of zero elements, which is common in text-based datasets. <br> 

<br> The sparse matrix is then converted into an array of 1s and 0s <br> 

Now, ML models can be trained on the sparse matrix to classify as "Real" or "Fake" News. <br> 

<br> **Step 8**: Training and Testing: The text corpus was trained on ensemble models like KNN, Random Forest, Decision Trees and powerful ML models like AdaBoost and GradientBoostingClassifiers. <br> 


<br> Random Forest correctly classified News as "Real" and "Fake" with an accuracy of 0.85. <br> 
